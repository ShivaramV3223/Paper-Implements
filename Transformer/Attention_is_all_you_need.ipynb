{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45056b67-7ca3-4e09-b1d9-16f5d2f66445",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "This jupyter notebook consist of the `Transformer` model implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae0e5c-de9a-4d54-96cb-26a9c9dcb300",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd302f9e-ba79-49cd-854b-33e011e30f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import Multi30k\n",
    "import spacy\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce7319-1e18-4441-80ca-2e7067b0af08",
   "metadata": {},
   "source": [
    "## Attention Layer Module\n",
    "Given an input X with dimensions ($N \\times d_{dim}$),  into Query Q($N \\times d_{q}$),  Keys K($N \\times d_{k}$) and  Values V($N \\times d_{v}$).\n",
    "Three different types of attention layers found in Transformer:\n",
    " - Encoder Self-Attention layer\n",
    " - Encoder-Decoder Attention layer\n",
    " - Decoder Self-Attention with Mask\n",
    " \n",
    "Encoding is calculated as follows:\n",
    "$$Z = softmax(\\frac{Q * K^{\\top}}{\\sqrt{d_{K}}})*V$$\n",
    "\n",
    "With Mask:\n",
    "$$Z = softmax(\\frac{Q * K^{\\top}}{\\sqrt{d_{K}}} + M)*V$$\n",
    "where M $\\rightarrow$ Mask(Look Ahead mask is implemented here)\n",
    "\n",
    "For Encoder Decoder attention layer, we have the Query coming from the Decoder and keys & values coming from the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84362b25-d720-4450-9e57-1a52cb3b8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Parameters\n",
    "        self.dq = params['dq']\n",
    "        self.dk = params['dk']\n",
    "        self.dv = params['dv']\n",
    "        self.dim = params['dim']\n",
    "        self.mask = params['mask']\n",
    "        \n",
    "        # Making Fully connected layers for calculating Queries, Values and Keys\n",
    "        self.Query = nn.Linear(self.dim, self.dq, bias = False)\n",
    "        self.Key = nn.Linear(self.dim, self.dk, bias = False)\n",
    "        self.Value = nn.Linear(self.dim, self.dv, bias = False)\n",
    "        \n",
    "    # Look ahead mask function\n",
    "    def Mask(self, nt, ns, batch_size):\n",
    "        \"\"\"nt -> Source sequence length\n",
    "           ns -> Target sequence length\"\"\"\n",
    "        \n",
    "        mask = torch.triu(torch.ones((nt, ns))).expand(batch_size, nt, ns)\n",
    "        mask = mask * (- float(\"inf\"))\n",
    "        return mask\n",
    "    \n",
    "    # Function to calculate the Q, K, V values\n",
    "    def QKV(self, X):\n",
    "        \"\"\"X -> (batch_size, seq_len, dim)\n",
    "           Q -> (batch_size, seq_len, dq)\n",
    "           K -> (batch_size, seq_len, dk)\n",
    "           V -> (batch_size, seq_len, dv)\"\"\"\n",
    "        \n",
    "        Q = self.Query(X)\n",
    "        K = self.Key(X)\n",
    "        V = self.Value(X)\n",
    "        \n",
    "        return Q, K, V\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, Xe, Xd = None):\n",
    "        Q, K, V = self.QKV(Xe)\n",
    "        if  Xd != None:\n",
    "            Q, k, v = self.QKV(Xd)\n",
    "        \n",
    "        Kt = K.permute(0, 2, 1)\n",
    "        I = Q @ Kt\n",
    "        \n",
    "        # Adding mask\n",
    "        if self.mask:\n",
    "            ns = Xe.shape[1]\n",
    "            nt = ns\n",
    "            if Xd != None:\n",
    "                nt = Xd.shape[1]\n",
    "                \n",
    "            I.masked_fill(self.Mask(nt, ns, X.shape[0]))\n",
    "        \n",
    "        # Calculating Score for the next layer\n",
    "        Z = (torch.softmax(I / self.dk ** (1/2), dim = 2 )) @ V\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13c43a-d434-47dd-93e2-729d6efc7bd8",
   "metadata": {},
   "source": [
    "##  Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6ac39a-86b2-42d4-84cd-3d2259c4d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters for the feed forward layer\n",
    "        self.in_size = params['input_size']\n",
    "        self.out_size = params['output_size']\n",
    "        self.hlayers = params['hlayers']\n",
    "        \n",
    "        # Making layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        cdim = self.in_size\n",
    "        \n",
    "        for h in self.hlayers:\n",
    "            self.layers.append(nn.Linear(cdim, h))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            \n",
    "            cdim = h\n",
    "        \n",
    "        self.layers.append(nn.Linear(cdim, self.out_size))\n",
    "        \n",
    "    # Forward fucntion\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8688a49-8ec2-437f-aec3-b9c3f66104ea",
   "metadata": {},
   "source": [
    "## MultiHead Attention\n",
    "\n",
    "We use several layers of self attention layers and concatenate their output to get the final encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822ab752-dcda-496d-a447-51e3f23f22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_Attention(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters for multi head attention\n",
    "        self.dim = params['dim']\n",
    "        self.dq = params['dq']\n",
    "        self.dk = params['dk']\n",
    "        self.dv = params['dv']\n",
    "        self.h = params['h']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.mask = params['mask']\n",
    "        \n",
    "        # calculating the number of dimensions per head\n",
    "        self.head_dim = self.dim // self.h\n",
    "        \n",
    "        # Parameters for individual head\n",
    "        self_attn_params = {\n",
    "            'dim' : self.head_dim,\n",
    "            'dq'  : self.dq,\n",
    "            'dk'  : self.dk,\n",
    "            'dv'  : self.dv,\n",
    "            'mask' : self.mask\n",
    "        }\n",
    "        \n",
    "        # Creating multiple heads\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.h):\n",
    "            self.layers.append(Attention_layer(self_attn_params))\n",
    "        \n",
    "        # Layer to transform into a single output\n",
    "        self.WO = nn.Linear(self.dv * self.h, self.dim, bias = False)\n",
    "        \n",
    "    def forward(self, Xe, Xd = None):\n",
    "        Z = []\n",
    "        \n",
    "        # Splitting the input \n",
    "        ns = Xe.shape[1]\n",
    "        Xe = Xe.view(self.h, self.batch_size, ns, self.head_dim) #dimension --> (heads, N, seq_len, head_dim)\n",
    "        \n",
    "        if Xd != None:\n",
    "            # Splitting the input\n",
    "            nt = Xd.shape[1]\n",
    "            Xd = Xd.view(self.h, self.batch_size, nt, self.head_dim) #dimension --> (heads, N, seq_len, head_dim)\n",
    "            \n",
    "            # Calculating the output of each head\n",
    "            for h in range(self.h):\n",
    "                z = self.layers[h](Xe[h], Xd[h])\n",
    "                Z.append(z)\n",
    "        else:\n",
    "            for h in range(self.h):\n",
    "                z = self.layers[h](Xe[h], Xd)\n",
    "                Z.append(z)\n",
    "        \n",
    "        # Concatenating the outputs\n",
    "        Z = torch.cat(Z, dim = 2) #dimension --> (N, seq_len, embed_dim)\n",
    "        # Final output\n",
    "        Z = self.WO(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201acbcb-7a1d-4334-9502-53fd966f9647",
   "metadata": {},
   "source": [
    "# Encoder Block\n",
    "\n",
    "An encoder block contains the following:\n",
    "- Multi-Head Attention Layer\n",
    "- Add Norm layer(Residual Connection)\n",
    "- Feed Forward Layer\n",
    "- Add Norm Layer\n",
    "\n",
    "<img src=\"./Encoder.png\" align=\"center\" width=200>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1715cc7-b705-45e8-9f4a-76546049ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters required for Multi Head Attention\n",
    "        self.dim = params['dim']\n",
    "        self.dq = params['dq']\n",
    "        self.dk = params['dk']\n",
    "        self.dv = params['dv']\n",
    "        self.h = params['h']\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "       \n",
    "        mult_attn_params = {\n",
    "            'dim' : self.dim,\n",
    "            'dq' : self.dq,\n",
    "            'dk' : self.dk,\n",
    "            'dv' : self.dv,\n",
    "            \"h\"  : self.h,\n",
    "            \"batch_size\" : self.batch_size,\n",
    "            'mask' : False\n",
    "        }\n",
    "        \n",
    "        # Parameters required for Feed Forward\n",
    "        self.out_size = params['output_size']\n",
    "        self.hlayers = params['hlayers']\n",
    "        \n",
    "        ffn_params ={\n",
    "            'input_size' : self.dim,\n",
    "            'output_size' : self.out_size,\n",
    "            \"hlayers\" : self.hlayers\n",
    "        }\n",
    "        \n",
    "        self.mult_heads = MultiHead_Attention(mult_attn_params)\n",
    "        self.norm1 = nn.LayerNorm(self.dim)\n",
    "        self.ffn = FeedForward(ffn_params)\n",
    "        self.norm2 = nn.LayerNorm(self.dim)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # MultiHead Attention along with residual and normalization\n",
    "        attn = self.mult_heads(X, None)\n",
    "        Z = self.norm1(attn + X)\n",
    "        \n",
    "        # Feed Forward Layer with residual connection and normalization\n",
    "        f = self.ffn(Z)\n",
    "        X = self.norm2(Z + f)\n",
    "        \n",
    "        return X    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e0fe5-59d1-4aed-8705-e1d2e989878c",
   "metadata": {},
   "source": [
    "# Decoder Block\n",
    "Block consists of the following:\n",
    "- Masked MultiHead Attention\n",
    "- Encoder Decoder Attention\n",
    "- Feed Forward Network\n",
    "\n",
    "<img src=\"./Decoder.png\" align=\"center\" width=200>\n",
    "Everything is associated with a residual connection and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36de095-4eae-4b31-9494-91a38dc142bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # params for multihead attention\n",
    "        self.dim = params['dim']\n",
    "        self.dq = params['dq']\n",
    "        self.dk = params['dk']\n",
    "        self.dv = params['dv']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.h = params['h']\n",
    "        \n",
    "        # Parameters for masked self attention\n",
    "        masked_mult_head = {\n",
    "            'dim' : self.dim,\n",
    "            'dq' : self.dq,\n",
    "            'dk' : self.dk,\n",
    "            \"dv\" : self.dv,\n",
    "            'h'  : self.h,\n",
    "            'batch_size' : self.batch_size,\n",
    "            'mask':True\n",
    "        }\n",
    "        \n",
    "        # Parameters for encoder decoder attention\n",
    "        enc_dec_mult_head = {\n",
    "            'dim' : self.dim,\n",
    "            'dq'  : self.dq,\n",
    "            'dk' : self.dk,\n",
    "            'dv' : self.dv,\n",
    "             'h' : self.h,\n",
    "            'batch_size' : self.batch_size,\n",
    "            'mask' : False\n",
    "        }\n",
    "        \n",
    "        # params for FFN\n",
    "        self.out_size = params['output_size']\n",
    "        self.hlayers = params['hlayers']\n",
    "        \n",
    "        ffn_params = {\n",
    "            'input_size' : self.dim,\n",
    "            'output_size' : self.out_size,\n",
    "            'hlayers' : self.hlayers\n",
    "        }\n",
    "        \n",
    "        # 3 sublayers in Decoder Block\n",
    "        self.mask_mult = MultiHead_Attention(masked_mult_head)\n",
    "        self.norm1 = nn.LayerNorm(self.dim)\n",
    "        self.encdec_mult = MultiHead_Attention(enc_dec_mult_head)\n",
    "        self.norm2 = nn.LayerNorm(self.dim)\n",
    "        self.ffn = FeedForward(ffn_params)\n",
    "        self.norm3 = nn.LayerNorm(self.dim)\n",
    "        \n",
    "    def forward(self, Xd, Xe):\n",
    "        \n",
    "        mask_attn = self.mask_mult(Xd, None)\n",
    "        Z = self.norm1(Xd + mask_attn)\n",
    "        \n",
    "        enc_dec_attn = self.encdec_mult(Xe, Z)\n",
    "        f = self.norm2(Z + enc_dec_attn)\n",
    "        \n",
    "        ffn = self.ffn(f)\n",
    "        X = self.norm3(f + ffn)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab91828-37f0-4e5f-bb7c-8590fd7ec4cd",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "Positional Encoding given in the paper:\n",
    "$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$\n",
    "where pos $\\rightarrow$ position and i $\\rightarrow$ dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05064c65-e783-4f4f-977e-166d01d154a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Positional_Encoding(X, embed_dim):\n",
    "    n, seq_len = X.shape\n",
    "    position_embedding = nn.Embedding(seq_len, embed_dim)\n",
    "    positions = torch.arange(seq_len).expand(n, seq_len)\n",
    "    PE = position_embedding(positions)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d52213-538a-403e-8e96-09cd9e28db93",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Here comes the transformer as a whole.\n",
    "We are building the transformer using the building blocks we created.\n",
    "\n",
    "<img src=\"./Transformer.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f022a46f-e2ec-4935-97f4-b960bf385d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of Encoder and Decoder blocks\n",
    "        self.N = params['N']\n",
    "        \n",
    "        # Params for Encoder Decoder\n",
    "        self.dim = params['dim']\n",
    "        self.dq = params['dq']\n",
    "        self.dk = params['dk']\n",
    "        self.dv = params['dv']\n",
    "        self.h = params['h']\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "        # Params for embedding\n",
    "        self.src_vocab_size = params['src_vocab_size']\n",
    "        self.trg_vocab_size = params['trg_vocab_size']\n",
    "        \n",
    "        # Params for FFN encoder\n",
    "        self.out_size1 = params['output_size_e']\n",
    "        self.hlayers1 = params['hlayers_e']\n",
    "        # Params for FFN decoder\n",
    "        self.out_size2 = params['output_size_d']\n",
    "        self.hlayers2 = params['hlayers_d']\n",
    "            \n",
    "        enc_params = {\n",
    "            'dim' : self.dim,\n",
    "            'dq' : self.dq,\n",
    "            'dk' : self.dk,\n",
    "            'dv' : self.dv,\n",
    "            'h' : self.h,\n",
    "            \"batch_size\" : self.batch_size,\n",
    "            'output_size' : self.out_size1,\n",
    "            'hlayers' : self.hlayers1\n",
    "        }\n",
    "        \n",
    "        dec_params = {\n",
    "            'dim' : self.dim,\n",
    "            'dq' : self.dq,\n",
    "            'dk' : self.dk,\n",
    "            'dv' : self.dv,\n",
    "            'h' : self.h,\n",
    "            'batch_size' : self.batch_size,\n",
    "            'output_size' : self.out_size2,\n",
    "            'hlayers' : self.hlayers2\n",
    "        }\n",
    "        \n",
    "        # Embedding\n",
    "        self.src_embedding = nn.Embedding(self.src_vocab_size, self.dim)\n",
    "        self.trg_embedding = nn.Embedding(self.trg_vocab_size, self.dim)\n",
    "        \n",
    "        # Encoder Blocks & Decoder Blocks:\n",
    "        self.EncoderBlocks = nn.ModuleList()\n",
    "        self.DecoderBlocks = nn.ModuleList()\n",
    "        \n",
    "        # Creating Encoder And Decoder Blocks\n",
    "        for n in range(self.N):\n",
    "            self.EncoderBlocks.append(EncoderBlock(enc_params))\n",
    "            self.DecoderBlocks.append(DecoderBlock(dec_params))\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        # X,Z -> Input Embedding, Output Embedding\n",
    "        X = self.src_embedding(src)\n",
    "        Z = self.trg_embedding(trg)\n",
    "        \n",
    "        # Adding positional Encoding\n",
    "        X += Positional_Encoding(X, self.dim)\n",
    "        Z += Positional_Encoding(Z, self.dim)\n",
    "        \n",
    "        # Passing through Encoders\n",
    "        for n in range(self.N):\n",
    "            X = self.EncoderBlocks[n](X)\n",
    "        \n",
    "        # Passing through Decoders\n",
    "        for n in range(self.N):\n",
    "            Z = self.DecoderBlocks[n](Z, X)\n",
    "        \n",
    "        # Passing through final Linear Layer\n",
    "        Z = self.output_layer(Z)\n",
    "        Z = F.softmax(Z)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd22da9-c485-4bf6-9994-35b711934a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_params = {\n",
    "    'dim' : 512,\n",
    "    'dq' : 64,\n",
    "    'dk' : 64,\n",
    "    'dv' : 64,\n",
    "    'h' : 8,\n",
    "    'N' : 6,\n",
    "    'batch_size' : 32,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cde2dd-a522-46db-bc0c-6c9245ef566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "mujoco_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
